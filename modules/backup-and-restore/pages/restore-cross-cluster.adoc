= Restore a Database Backup from Another Cluster
:page-aliases: cross-cluster-backup.adoc
:description:

This page walks you through the process of restoring a cluster from a backup that was created from another database cluster.

Restoring is an off-line operation.
Your cluster is offline during the restore process.

== Prerequisites
* Your new cluster has the same xref:cluster-and-ha-management:ha-cluster.adoc[partitioning factor] as the cluster from which the backup file was created.
* TigerGraph is installed on the new cluster in the exact same version as the previous cluster.
* You have access to the TigerGraph Linux user account on your cluster.
All commands must be run from the TigerGraph Linux user.
* You have access to a data backup from another cluster.
* [xref:_procedure_v3_9_1_and_below[3.9.1 and earlier]] You have the metadata backup that goes with the data backup.
* You have xref:configurations.adoc[configured backup and restore] on the new cluster.

== Procedure

=== 1) Copy Data Backup to a New Cluster

==== For Amazon S3 Backup:

If your backups are stored in Amazon S3 buckets, set the following configuration parameter to be the location of your backup files.

[source,console]
System.Backup.S3.BucketName

For the new cluster to be the same as the previous cluster.

==== For Local Backup Storage:

If your backups were stored locally, you need to make sure backup files from all nodes are copied to the new cluster.
They do not necessarily need to be on the corresponding nodes as long as all partitions are in the new cluster at the specified paths you configured.

* For example:
+
.If you have configured the backup path to be
[source,console]
/home/tigergraph/backups
+
.for a 3-node cluster, you can put all backup files for backup
[source,console]
weekly-<timestamp>
+
.in the folder like this:
[source,console]
/home/tigergraph/backups/weekly-<timestamp>

=== 2) Restore Cluster

Now, just like restoring a backup in its original cluster, simply run:
[source, console]
gadmin backup restore <backup_tag>

== Procedure v3.9.1 and below

=== 1) Upload Metadata to New Cluster
Upload the metadata file specific to the backup with which you want to restore the cluster to a path on any node of the new cluster where the TigerGraph Linux user has access.

For example, upload the  file to
[source, console]
/home/tigergraph/metadata

on m1 of the new cluster.

=== 2) Copy Data Backup to a New Cluster

==== For Amazon S3 Backup:

If your backups are stored in Amazon S3 buckets, set the following configuration parameter to be the location of your backup files.

[source,console]
System.Backup.S3.BucketName

For the new cluster to be the same as the previous cluster.

==== For Local Backup Storage:

If your backups were stored locally, you need to make sure backup files from all nodes are copied to the new cluster.
They do not necessarily need to be on the corresponding nodes as long as all partitions are in the new cluster at the specified paths you configured.

* For example:
+
.If you have configured the backup path to be
[source,console]
/home/tigergraph/backups
+
.for a 3-node cluster, you can put all backup files for backup
[source,console]
weekly-<timestamp>
+
.in the folder like this:
[source,console]
/home/tigergraph/backups/weekly-<timestamp>

=== 3) Restore Cluster

Run the following command to restore the cluster using the backup.

[IMPORTANT]
You must run the command on the cluster to which you uploaded the cluster metadata.

[.wrap,console]
----
$ gadmin backup restore --meta=/home/tigergraph/metadata <1>
----
<1> If you uploaded the metadata to a different folder, replace the path with the path where the file is.