= Load from Spark Dataframe

The TigerGraph Spark Connector is used with
https://spark.apache.org[Apache Spark] to read data from a Spark DataFrame (or Data Lake) and write to TigerGraph.

As of TigerGraph 4.1, it can also be used from Spark to run a TigerGraph query and xref:data-loading:read-to-spark-dataframe.adoc[streaming data into Spark] as a Dataframe.

Users can leverage it to connect TigerGraph to the Spark ecosystem and load data from any Spark data sources, e.g.:

* Distributed file system: HDFS, S3, GCS and ABS
* Streaming source: Kafka
* Data warehouse:
xref:tigergraph-server:data-loading:load-from-warehouse.adoc#_bigquery[BigQuery],
xref:tigergraph-server:data-loading:load-from-warehouse.adoc#_snowflake[Snowflake],
xref:tigergraph-server:data-loading:load-from-warehouse.adoc#_postgresql[PostgreSql],
and Redshift
* Open table format:
xref:tigergraph-server:data-loading:load-from-spark-dataframe.adoc#_load_data_from_delta_lake[Delta Lake],
xref:tigergraph-server:data-loading:load-from-spark-dataframe.adoc#_load_data_from_iceberg[Iceberg]
and xref:tigergraph-server:data-loading:load-from-spark-dataframe.adoc#_load_data_from_hudi[Hudi]

This connector has multiple optimizations and is backward compatible with TigerGraph v3.6.0+.

* Job level loading statistics is available in v3.10+.
* Support for reading data into Spark from TigerGraph is available in v4.1+.

include::partial$spark/jdbc-deprecation.adoc[]

== Prerequisites

include::partial$spark/prerequisite.adoc[]

=== Create a Loading Job
[source, gsql]
----

CREATE VERTEX Comment (PRIMARY_ID id UINT, creationDate DATETIME,
  locationIP STRING, browserUsed STRING, content STRING, length UINT)
  WITH primary_id_as_attribute="TRUE", STATS="outdegree_by_edgetype"
CREATE GRAPH demo_graph (*)

USE GRAPH demo_graph
CREATE LOADING JOB load_Comment FOR GRAPH demo_graph {
    DEFINE FILENAME file_Comment;
    LOAD file_Comment
        TO VERTEX Comment VALUES ($1, $0, $2, $3, $4, $5)
        USING header="true", separator="|";
}
----

== Overview
TigerGraph Spark loading leverages  xref:tigergraph-server:API:built-in-endpoints.adoc#_run_a_loading_job[TigerGraph loading job's built-in endpoint].
It requires a pre-defined loading job, then the dataframe will be processed as follows:

.Spark dataframe:
[console]
----
+--------------------+-------------+-------------+-----------+--------------------+------+
|        creationDate|           id|   locationIP|browserUsed|             content|length|
+--------------------+-------------+-------------+-----------+--------------------+------+
|2012-07-04T06:10:...|7696585588755| 46.23.82.182|    Firefox|                 LOL|     3|
|2012-08-22T17:22:...|8246341402699|  27.62.125.4|     Chrome|              roflol|     6|
|2012-05-08T21:02:...|7146829775042|  61.1.50.205|     Chrome|              roflol|     6|
|2012-11-22T01:25:...|9345853030654|190.95.68.192|    Firefox|About Sergei Eise...|    79|
|2012-11-11T08:59:...|9345853030710|166.75.225.76|     Chrome|                good|     4|
+--------------------+-------------+-------------+-----------+--------------------+------+
----

.The connector concatenates columns to delimited data.
If the following options are used,

- loading.separator = "|"
- loading.eol = "\n"

then the processed data would be
[console]
----
2012-07-04T06:10:43.489+00:00|7696585588755|46.23.82.182|Firefox|LOL|3
2012-08-22T17:22:20.315+00:00|8246341402699|27.62.125.4|Chrome|roflol|6
2012-05-08T21:02:39.145+00:00|7146829775042|61.1.50.205|Chrome|roflol|6
2012-11-22T01:25:39.670+00:00|9345853030654|190.95.68.192|Firefox|About Sergei Eisenstein, pioneering SAbout Steven Spielberg, makers in thAbout|79
2012-11-11T08:59:21.311+00:00|9345853030710|166.75.225.76|Chrome|good|4
----

.The processed data will be sent to TigerGraph as CSV file sources for the loading job:
- loading.job = "load_Comment"
- loading.filename = "file_Comment"

To let TigerGraph parse the data correctly, please make sure you are setting `loading.separator` and `loading.eol` to an unique character.

== Usage
include::partial$spark/pre_def_options.adoc[]

[NOTE]
.Authentication
====
Authenticate with the username/password or secret to automatically maintain a valid token; otherwise, make sure the lifetime of your token is long enough for the loading job.
====

=== Batch Write

.Read data from any Spark data sources into the dataframe:
[source, scala]
----
val df = spark.read.json("path/to/Comment.json")
----

.Batch write the data into TigerGraph:
[source, scala]
----
df.write
    .format("tigergraph")
    .mode("append")
    .options(tgOptions)
    .option("loading.job", "load_Comment")
    .option("loading.filename", "file_Comment")
    .option("loading.separator", "|")
    .option("log.level", "2")
    .option("log.file", "/tmp/tigergraph-spark-connector.log")
    .save()
----

=== Write with Spark Structured Streaming API

.Read data from any Spark streaming data sources into the dataframe:
[source, scala]
----
val df = spark.readStream
    .format("kafka")
    .option("subscribe", "Comment")
    .load()
    .selectExpr("CAST(value AS STRING)").as[(String)]
----

.Streaming write data to TigerGraph:
[source, scala]
----
df.writeStream
    .outputMode("append")
    .format("tigergraph")
    .option("checkpointLocation", "/path/to/checkpoint")
    .options(tgOptions)
    .option("loading.job", "load_Comment")
    .option("loading.filename", "file_Comment")
    .option("loading.separator", "|")
    .option("log.level", "2")
    .option("log.file", "/tmp/tigergraph-spark-connector.log")
    .start()
    .awaitTermination()
----

== Connector Options
include::partial$spark/common-options.adoc[]
=== Writer Options
[separator=¦ ]
|===
¦ Key ¦ Default Value ¦  Description ¦ Group

¦ `loading.job`
¦ (none)
¦ The GSQL loading job name.
¦ Loading Job

¦ `loading.filename`
¦ (none)
¦ The filename defined in the loading job.
¦ Loading Job

¦ `loading.separator`
¦ ,
¦ The column separator.
¦ Loading Job

¦ `loading.eol`
¦ \n
¦ The line separator.
¦ Loading Job

¦ `loading.batch.size.bytes`
¦ 2097152
¦ The maximum batch size in bytes.
¦ Loading Job

¦ `loading.timeout.ms`
¦ (none)
¦ The loading timeout per batch.
¦ Loading Job

¦ `loading.max.percent.error`
¦ (none)
¦ The threshold of the error objects count.
The loading job will be aborted when reaching the limit.
Only available for TigerGraph version 3.10.0+.
¦ Loading Job

¦ `loading.max.num.error`
¦ (none)
¦ The threshold of the error objects percentage.
The loading job will be aborted when reaching the limit.
Only available for TigerGraph version 3.10.0+.
¦ Loading Job

¦ `loading.retry.interval.ms`
¦ 5000
¦ The initial retry interval for transient server errors.
¦ Loading Job

¦ `loading.max.retry.interval.ms`
¦ 30000
¦ The maximum retry interval for transient server errors.
¦ Loading Job

¦ `loading.max.retry.attempts`
¦ 10
¦ The maximum retry attempts for transient server errors.
¦ Loading Job
|===

== Use Cases

=== Load Data from Delta Lake
==== Batch Write

.Load delta table to Spark dataframe:
[source, scala]
----
val df = spark.read.format("delta")
    .load("/path/to/delta/table")
    .select(
        "creationDate",
        "id",
        "locationIP",
        "browserUsed",
        "content",
        "length"
    )
----

.Batch write the data into TigerGraph:
[source, scala]
----
df.write
    .format("tigergraph")
    .mode("append")
    .options(tgOptions)
    .option("loading.job", "load_Comment")
    .option("loading.filename", "file_Comment")
    .option("loading.separator", "|")
    .option("log.level", "2")
    .option("log.file", "/tmp/tigergraph-spark-connector.log")
    .save()
----

==== Streaming Write(CDC)

.Streaming read from delta table:
[source, scala]
----
val df = spark.readStream
    .format("delta")
    .option("readChangeFeed", "true")
    .load("/path/to/delta/table")
    .filter(
        $"_change_type" === "insert" || $"_change_type" === "update_postimage"
    )
    .select(
        "creationDate",
        "id",
        "locationIP",
        "browserUsed",
        "content",
        "length"
    )
----

.Streaming write data to TigerGraph:
[source, scala]
----
df.writeStream
    .outputMode("append")
    .format("tigergraph")
    .option("checkpointLocation", "/path/to/checkpoint")
    .options(tgOptions)
    .option("loading.job", "load_Comment")
    .option("loading.filename", "file_Comment")
    .option("loading.separator", "|")
    .start()
    .option("log.level", "2")
    .option("log.file", "/tmp/tigergraph-spark-connector.log")
    .awaitTermination()
----

For more details on Delta Lake see https://docs.delta.io/latest/index.html[Welcome to the Delta Lake documentation — Delta Lake Documentation].

=== Load Data from Iceberg
==== Batch Write

.Load Iceberg table to Spark dataframe:
[source, scala]
----
val df = spark.table("catalog.db.table")
    .select(
        "creationDate",
        "id",
        "locationIP",
        "browserUsed",
        "content",
        "length"
    )
----

.Batch write the data into TigerGraph:
[source, scala]
----
df.write
    .format("tigergraph")
    .mode("append")
    .options(tgOptions)
    .option("loading.job", "load_Comment")
    .option("loading.filename", "file_Comment")
    .option("loading.separator", "|")
    .option("log.level", "2")
    .option("log.file", "/tmp/tigergraph-spark-connector.log")
    .save()
----

==== Streaming Write(CDC)

.Streaming read from Iceberg table:
[source, scala]
----
val df = spark.readStream
    .format("iceberg")
    .option("stream-from-timestamp", 0L)
    .load("catalog.db.table")
    .select(
        "creationDate",
        "id",
        "locationIP",
        "browserUsed",
        "content",
        "length"
    )
----

.Streaming write data to TigerGraph:
[source, scala]
----
df.writeStream
    .outputMode("append")
    .format("tigergraph")
    .option("checkpointLocation", "/path/to/checkpoint")
    .options(tgOptions)
    .option("loading.job", "load_Comment")
    .option("loading.filename", "file_Comment")
    .option("loading.separator", "|")
    .option("log.level", "2")
    .option("log.file", "/tmp/tigergraph-spark-connector.log")
    .start()
    .awaitTermination()
----

For more details on Iceberg see https://iceberg.apache.org/docs/1.3.1/getting-started/[Iceberg Apache: Getting Started]

=== Load Data from Hudi
==== Batch Write

.Load Hudi table to Spark dataframe:
[source, scala]
----
val df = spark.read
    .format("hudi")
    .load("/path/to/hudi/table")
    .select(
        "creationDate",
        "id",
        "locationIP",
        "browserUsed",
        "content",
        "length"
    )
----

.Batch write the data into TigerGraph
[source, scala]
----
df.write
    .format("tigergraph")
    .mode("append")
    .options(tgOptions)
    .option("loading.job", "load_Comment")
    .option("loading.filename", "file_Comment")
    .option("loading.separator", "|")
    .option("log.level", "2")
    .option("log.file", "/tmp/tigergraph-spark-connector.log")
    .save()
----

==== Streaming Write(CDC)

.Streaming read from Hudi table:
[source, scala]
----
val df = spark.readStream
    .format("hudi")
    .load("/path/to/hudi/table")
    .select(
        "creationDate",
        "id",
        "locationIP",
        "browserUsed",
        "content",
        "length"
    )
----

.Streaming write data to TigerGraph:
[source, scala]
----
df.writeStream
    .outputMode("append")
    .format("tigergraph")
    .option("checkpointLocation", "/path/to/checkpoint")
    .options(tgOptions)
    .option("loading.job", "load_Comment")
    .option("loading.filename", "file_Comment")
    .option("loading.separator", "|")
    .option("log.level", "2")
    .option("log.file", "/tmp/tigergraph-spark-connector.log")
    .start()
    .awaitTermination()
----

For more details on Hudi see https://hudi.apache.org/docs/quick-start-guide/[Spark Guide | Apache Hudi].

== Loading Statistics
When you configure the logging properly and set log level to info, the loading statistics will be logged.

There are 3 levels of stats:

* *Batch level*: data will be loaded to TigerGraph by micro batches, malformed or invalid data count of the batch will be logged.
* *Partition level*: the data source can contain multiple partitions, and the log will show how many rows of the partition has been sent to TigerGraph.
* *Job Level (only available for TigerGraph 3.10.0 or higher)*: The overall loading statistics of the Spark job aggregated by TigerGraph service KAFKASTRM-LL. It requires providing username and password to query /gsqlserver endpoint.

.Sample loading statistics:
[source, scala]
----
24/01/22 16:15:45 INFO TigerGraphBatchWrite: Overall loading statistics: [ {
    "overall" : {
        "duration" : 15792,
        "size" : 48675207,
        "progress" : 0,
        "startTime" : 1706770863875,
        "averageSpeed" : 29546,
        "id" : "test_graph.load_Comment.spark.all.1706770859889",
        "endTime" : 1706770879667,
        "currentSpeed" : 29546,
        "statistics" : {
            "fileLevel" : {
                "validLine" : 466594,
                "notEnoughToken" : 0,
                "tokenExceedsBuffer" : 0,
                "rejectLine" : 0
            },
            "objectLevel" : {
                "vertex" : [ {
                "validObject" : 466593,
                "typeName" : "Comment",
                "invalidPrimaryId" : 1
                } ]
            }
        }
    },
    "workers" : [ {
        "tasks" : [ {
            "filename" : "file_Comment"
        } ]
    }, {
    "tasks" : [ {
        "filename" : "file_Comment"
        } ]
    } ]
} ]
----

== Level Statistic Reference

=== Row Level Statistics
[separator=¦ ]
|===
¦ Row Level Statistics ¦ Description

¦ `validLine` ¦ Number of valid raw data lines parsed.
¦ `rejectLine` ¦ Number of raw data lines rejected by the reject line rule in the loading script.
¦ `notEnoughToken` ¦ Number of raw data lines with fewer tokens than what was specified by the loading script.
¦ `badCharacter` ¦ Number of raw data lines containing invalid characters.
¦ `tokenExceedsBuffer` ¦ Number of raw data lines containing oversize tokens (see `gadmin config get GSQL.OutputTokenBufferSize`).
¦ `emptyLine` ¦ Number of raw data lines that are empty.

|===

=== Object Level Statistics
[separator=¦ ]
|===
¦ Object Level Statistics ¦ Description

¦ `validObject` ¦ Number of data records created.
¦ `passedCondition` ¦ Number of token lists which passed the WHERE predicate filter.
¦ `failedCondition` ¦ Number of token lists which failed the WHERE predicate filter.
¦ `invalidPrimaryId` ¦ Number of token lists where the id token is invalid.
¦ `noIdFound` ¦ Number of token lists where the id token is empty.
¦ `invalidAttribute` ¦ Number of token lists where at least one of the attribute tokens is invalid.
¦ `incorrectFixedBinaryLength` ¦ Number of token lists where at least one of the tokens corresponding to a UDT type attribute is invalid.
¦ `invalidVertexType` ¦ Number of token lists where at least one of the tokens corresponding to an edge type's source/target vertex type is invalid.
|===
