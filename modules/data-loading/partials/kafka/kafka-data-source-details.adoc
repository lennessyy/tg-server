=== Configure the Kafka source

The TigerGraph connector to external Kafka sources makes use of https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330[Apache Kafka Mirrormaker].

[NOTE]
====
Users can additionally utilize external sources, including files, Vault, and environment variables, to provide configurations for Kafka connectors. See xref:tigergraph-server:data-loading:externalizing-kafka-configs.adoc[].
====

To configure the data source object, the minimum requirement is the address of the external source Kafka cluster:

[source,json,linenum]
.Data source configuration for external Kafka
----
{
"type": "mirrormaker",
"source.cluster.bootstrap.servers": "<broker_addrs>"
}
----

If the source cluster is configured for SSL or SASL protocols, you need to provide the following SSL/SASL credentials in order to communicate with the source cluster.

* If the source cluster uses SASL, you need to upload the keytab of each Kerberos principal to every node of your TigerGraph cluster at the same absolute path.
* If the source cluster uses SSL, see our documentation xref:tigergraph-server:data-loading:kafka-ssl-security-guide.adoc[]
* If the source cluster uses SASL *and* SSL, you need to upload the keytab of each Kerberos principal, as well as the key store and truststore to every node of your TigerGraph cluster.
Each file must be at the same absolute path on all nodes.

The following are generic configurations required for admin, producer and consumer. To supply the configuration for the corresponding component, replace `<prefix>` with `source.admin`, `producer`, or `consumer`.
For example, to specify `GSSAPI` as the SASL mechanism for consumer, include `"consumer.sasl.mecahnism": "GSSAPI"` in the data source configuration.
But if the source cluster uses SSL, please skip the generic configurations below and follow this documentation to setup connector xref:tigergraph-server:data-loading:kafka-ssl-security-guide.adoc[]

[%header,cols="1,2"]
|===
| Field | Description

| <prefix>.security.protocol
| Protocol used to communicate with brokers.
Valid values are: `PLAINTEXT`, `SSL, `SASL_PLAINTEXT`, `SASL_SSL`.
The default is `PLAINTEXT`.

| <prefix>.sasl.mechanism
| SASL mechanism used for client connections.
This may be any mechanism for which a security provider is available. GSSAPI is the default mechanism.

| <prefix>.sasl.kerberos.service.name
| The Kerberos principal name used by your Kafka brokers.
This could be defined in either JAAS configuration or Kafkaâ€™s configuration.

| <prefix>.sasl.jaas.config
| JAAS login context parameters for SASL connections in the format used by JAAS configuration files.
See https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/LoginConfigFile.html[JAAS Login Configuration File] for details.

| <prefix>.ssl.endpoint.identification.algorithm
| The endpoint identification algorithm used to validate server hostname in the server certificate. Default is `https`.
If the value is set to an empty string, this will disable server host name verification.

| <prefix>.ssl.keystore.location
| The location of the key store file.

| <prefix>.ssl.keystore.password
| The password of the key store file.

| <prefix>.ssl.key.password
| The password of the private key in the key store file or the PEM key specified in `ssl.keystore.key`.

| <prefix>.ssl.truststore.location
| The location of the trust store file.

| <prefix>.ssl.truststore.password
| The password for the trust store file.
|===

If there is a https://docs.confluent.io/platform/current/schema-registry/index.html[schema registry service] containing the record schema of the source topic, please add it to the data source configuration:

[source,json]
"value.converter.schema.registry.url": "schema_registry_url"

[NOTE]
Currently, only Avro schema is supported.

