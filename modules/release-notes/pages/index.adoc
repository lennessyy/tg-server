= TigerGraph DB Release Notes
:description: Release notes for TigerGraph {page-component-version} LTS.
//:page-aliases: change-log.adoc, release-notes.adoc
:fn-preview: footnote:preview[Features in the preview stage should not be used for production purposes. General Availability (GA) versions of the feature will be available in a later release.]
:pp: {plus}{plus}
:toc:
:toclevels:2


TigerGraph Server 3.10.1 LTS was released on May 7th, 2024.

TigerGraph Server 3.10.0 preview version was released on March 13th, 2024.

LTS versions are supported for 24 months from their initial release (X.X.0) and should be the choice for production deployments.

== Key New Features

* xref:tigergraph-server:system-management:change-data-capture/cdc-overview.adoc[Change Data Capture (CDC)] - This equips TigerGraph users with the capability to automatically capture and stream data changes to external Kafka systems maintained by the user.
Additionally, CDC can be xref:gui:admin-portal:components/cdc.adoc[configured] in xref:gui:admin-portal:overview.adoc[Admin Portal].

* xref:tigergraph-server:system-management:workload-management.adoc#_workload_queue[Workload Queue] - Configure workload queues so that queries are routed to the appropriate queues during runtime.

* xref:tigergraph-server:data-loading:load-from-spark-dataframe.adoc[Spark Connector] - A new dedicated connector used with Apache Spark to read data from a Spark DataFrame and write to TigerGraph.

* xref:tigergraph-server:backup-and-restore:online-backup.adoc[] - Minimizes blocking time during backups, allowing successful execution of post requests for TigerGraph users.

* xref:tigergraph-server:backup-and-restore:differential-backups.adoc[] - Ensure data files that have changed since the most recently completed full backup are backed up without any data lost.

* xref:tigergraph-server:installation:upgrade.adoc#_upgrading_from_3_x[Refined Upgrading process] - Changes to the upgrading process to allow a TigerGraph upgrade without having to upgrade the K8's operator.

* xref:tigergraph-server:installation:upgrade.adoc#_option_n[Support Non-Interactive Upgrade] - The user can use option `-n` to avoid input ( `y/n` ) to switch to the new version.

* xref:gsql-ref:ddl-and-loading:modifying-a-graph-schema.adoc#_global_vs_local_schema_changes[Global Schema Restrict to Global Scope] - Users must have the global scope to interact with global schema change jobs (create, delete, run).

== Detailed List of New and Modified Features

=== TigerGraph Server

* [3.10.1] xref:tigergraph-server:reference:configuration-parameters.adoc#_environment_variables[Two new configurations to help tune re-builder scheduling logic] -  When running `GPE.BasicConfig.Env.` there are two new configurations `SegmentMetaFlushAlways` and `SegmentMetaForceFlushIntervalSec` to help fine tune re-builder scheduling.

* [3.10.1] xref:tigergraph-server:installation:hw-and-sw-requirements.adoc#_certified_operating_systems[Ubuntu 22.04 is now certified] - Ubuntu 22.04 is now one of TigerGraph's certified OS systems.

* [3.10.1] xref:tigergraph-server:troubleshooting:audit-log.adoc#_rest_api_call_audit_logs[Audit log support for direct RESTPP API calls] - Audit logs now record direct REST++ API Calls.

* [3.10.1] Added new xref:tigergraph-server:reference:return-codes.adoc[Return Codes]:
** `REST-4000` - Response time exceeds the timeout limit.
** `REST-10020` - License has expired.
** `REST-10021` - Access to the file has failed.
** `REST-30001` - The parameter is invalid (general error).

* [3.10.1] xref:tigergraph-server:reference:configuration-parameters.adoc[New configuration field System.Metrics.IncludeHostName] - Users now have the option to include hostname/ip in the metrics output in OpenMetrics format.

* [3.10.1] xref:tigergraph-server:installation:bare-metal-install.adoc[Nodes can now be paired with hostnames in installation] - Users now can set the node by ip or hostname during installation.
This is supported in both interactive and non-interactive installation.

* [3.10.1] xref:cloud:solutions:access-solution/rest-requests.adoc#_secret_request[Make request with GSQL-Secret] -
users can now use the authorization secret in a request header as a GSQL-Secret.

* [3.10.1] xref:gsql-ref:appendix:keywords-and-reserved-words.adoc[Four new reserved keywords] - `FUNCTION`, `OPENCYPHER`, `POLICY`, and `ROW` have all been added to the DDL GSQL reserved words and keywords list.

* xref:tigergraph-server:troubleshooting:audit-log.adoc[] - Audit logs maintain a historical record of activity events, noting the time, responsible user or service, and affected entity.

* xref:tigergraph-server:backup-and-restore:single-graph-import-export.adoc[] - Users can now import or export individually selected graphs.

* xref:tigergraph-server:cluster-and-ha-management:set-up-crr.adoc[Updates to the Set Up CRR] - Updates to the Cross-Region Replication (CRR) setup enable this feature to flow into the `gadmin backup restore` process preventing a writable gap.

* xref:tigergraph-server:data-loading:externalizing-kafka-configs.adoc[Externalizing Kafka Configs using Config Provider] - TigerGraph’s Kafka connector config will use a reference to retrieve the config value from an external source.

* xref:tigergraph-server:system-management:management-commands.adoc#_gadmin_backup_create[Two new flags for gadmin backup create] - added two parameters to enable data consistency checks during the backup.

=== GSQL Command and Querying Language

//* [3.10.1] xref:gsql-ref:querying:func/aggregation-functions.adoc#_percentilecont[New Aggregation funtion percentileCont()] - `percentileCont()` Aggregating Returns the percentile of the given value over a group using linear interpolation.

//* [3.10.1] xref:gsql-ref:querying:accumulators.adoc#_percentilecontaccum[A new accumulator PercentileContAccum()] - `PercentileContAccum()` that returns the percentile of the given value over a group using linear interpolation.

* xref:gsql-ref:querying:func/context-functions.adoc[] - Context functions are a set of new built-in functions that provide insights into the user's information and work inside `INSTALLED` queries, `INTERPRET` queries, and xref:tigergraph-server:user-access:rbac-row-policy/rbac-row-policy.adoc#_gsql_functions[GSQL Functions].

* xref:tigergraph-server:system-management:management-commands.adoc[Command Updates] - Added new flag `--local` to commands xref:tigergraph-server:system-management:management-commands.adoc#_gadmin_start[gadmin start] and xref:tigergraph-server:system-management:management-commands.adoc#_gadmin_stop [gadmin stop] reducing the time required to start/stop local services.

* xref:_improvements[GSQL Data Streaming Improvements] - Improvements to GSQL data streaming that reduce the CPU usage, improve performance, optimizes Disk usage, and increases stability and cohesion.

==== Loading

* xref:tigergraph-server:data-loading:load-from-warehouse.adoc[Support for Snowflake Data Warehouse] - Added support for loading data from Snowflake, another popular data warehouse.

* xref:tigergraph-server:data-loading:load-from-warehouse.adoc[Support for PostgreSql Data Warehouse] - Added support for loading data from PostgreSql, another popular data warehouse.

* xref:tigergraph-server:data-loading:avro-validation-with-kafka.adoc[] - The KafkaConnect feature flag `ErrorTolerance` enables data loading services to handle malformed data and report errors effectively

* xref:tigergraph-server:cluster-and-ha-management:ha-overview.adoc#_file_and_kafka_loaders_ha_with_auto_restart[Local file and Kafka loader Auto-Restart] - Local file and Kafka loader will now auto restart if it unexpectedly quits.

==== Schema

* New non-interactive mode GSQL operations - Added four new non-interactive mode for operations xref:tigergraph-server:user-access:user-management.adoc#_non_interactive_mode[create user], xref:tigergraph-server:user-access:user-management.adoc#_non_interactive_mode[alter password], xref:tigergraph-server:backup-and-restore:database-import-export.adoc#_non_interactive_mode_export_graph_all[export graph all], and xref:tigergraph-server:backup-and-restore:database-import-export.adoc#_non_interactive_mode_import_graph_all[import graph all].

==== Querying and Query Management

* [3.10.1] xref:gsql-ref:ddl-and-loading:modifying-a-graph-schema.adoc#_impact_warning[New impact warnings when running schema change job] - Added a warning message when running the global or local schema change job so, users can understand the impact of running a global or local schema change on a query.

* xref:gsql-ref:querying:accumulators.adoc#_edge_accumulators[Support Edge Accumulators] - Support for edge accumulators for a single-hop distributed query.

* xref:gsql-ref:ddl-and-loading:modifying-a-graph-schema.adoc#_run_global_schema_change_job[Option -N to schema change job] - Added an option to `RUN SCHEMA_CHANGE JOB` that skips recompile and reinstall queries.

=== Kubernetes Operator

* [3.10.1] xref:tigergraph-server:kubernetes:k8s-operator/index.adoc[Kubernetes Operator is GA]  - The Kubernetes Operator is now in GA with 3.10.1.

* https://github.com/tigergraph/ecosys/blob/master/k8s/docs/07-reference/configure-tigergraph-cluster-cr-with-yaml-manifests.md[Added new fields] - `.spec.tigergraphConfig` in TigerGraph CR and a new option `--tigergraph-config` in kubectl-tg plugin.

* https://github.com/tigergraph/ecosys/blob/k8s-operator/0.1.0/k8s/docs/07-reference/configure-tigergraph-cluster-cr-with-yaml-manifests.md[Updates to cluster creation using YAML] - Improvements to the configurations to align with the database.

* https://github.com/tigergraph/ecosys/blob/k8s-operator/0.1.0/k8s/docs/03-deploy/multiple-persistent-volumes-mounting.md[Support for mounting multiple PVC and PV for pods] - Added two optional fields `additionalStorages` and `spec.storage` to customize the PV for TigerGraph pods.

* https://github.com/tigergraph/ecosys/blob/k8s-operator/0.1.0/k8s/docs/03-deploy/customize-tigergraph-pod.md[Support for customizing of pods] -  Customize the pods or containers, for example, users can add more customized labels and annotations or change the security context of the containers.

* https://github.com/tigergraph/ecosys/blob/k8s-operator/0.1.0/k8s/docs/07-reference/configure-tigergraph-cluster-cr-with-yaml-manifests.md[Pause a running cluster] -
Added a new field `.spec.pause` in TigerGraph CR and a new subcommand `kubectl tg pause` in `kubectl-tg plugin`.
Users scan set `.spec.pause=true` to pause a running cluster and resume it by setting `.spec.pause=false`.

=== Security

* [3.10.1] xref:tigergraph-server:user-access:sso.adoc#_match_strategy_extensions[SSO Match Strategy Extension] - SSO match strategy has been extended to allow matches via regular expression.

* [3.10.1] xref:tigergraph-server:user-access:jwt-token.adoc#_optional_configurations[Added a JWT Token config] - `Security.JWT.Audience` added to allow users
to set a JWT Token authentication that verifies if the `aud` (recipient for which the JWT is intended) defined in JWT Token matches the configured one or not.

* xref:tigergraph-server:user-access:rbac-row-policy/row-policy-overview.adoc[RBAC: Row Policy (Preview Feature)] - is used to control access to specific rows of data in TigerGraph.
See also xref:tigergraph-server:user-access:rbac-row-policy/row-policy-ebnf.adoc[] for examples.

* xref:tigergraph-server:user-access:rbac-row-policy/rbac-row-policy.adoc#_object_based_privileges[Object-Based Privileges] - This mechanism allows users to grant or revoke privileges based on specific objects.
See xref:tigergraph-server:user-access:rbac-row-policy/row-policy-privlages-table.adoc[] for a complete list.

* xref:tigergraph-server:user-access:jwt-token.adoc[] - Provides token-based authentication in JSON web token (JWT) format, allows TigerGraph users better control over application access.

* xref:tigergraph-server:security:gsql-file-input-policy.adoc[] - `GSQL.fileInputPolicy` allows users to apply restrictions on the location of local files used to load data to TigerGraph.

* xref:tigergraph-server:data-loading:kafka-ssl-security-guide.adoc[Kafka Security via SSL] - Kafka brokers can be secured by SSL including the connections from Kafka clients to Kafka brokers.

== TigerGraph Suite Updates
=== Admin Portal

* xref:gui:admin-portal:components/cdc.adoc[Change Data Capture (CDC)] can be enabled in xref:gui:admin-portal:overview.adoc[Admin Portal].

* xref:gui:admin-portal:security/sso-oidc-okta.adoc[SSO.OIDC via Okta] - Support for Standard OIDC Authorization Code Flow for general purpose adds more security for logins to Admin Portal Users.

=== GraphStudio

* xref:gui:graphstudio:export-and-import-solution.adoc[Single Graph Import and Export Support]  - Allow users to choose a single graph and the data when they export or import data in GraphStudio.

* xref:tigergraph-server:reference:configuration-parameters.adoc#_gui[New GUI command to disable concurrent sessions ] - `GUI.EnableConcurrentSession` allows users to disable concurrent sessions so that multiple browsers cannot log in with the same username at the same time, revoking the previous session and warning the user to re-login.

=== TigerGraph Insights

* xref:insights:widgets:single-value.adoc[Changing Single Value Widget to Value Widget]  - Modified the value element of insights to support the mapping of multiple values.
* xref:insights:widgets:markdown-widget.adoc[Added Markdown Widget] - This addition allows users to add formatted text, links, images, and other rich content to the dashboards.
* xref:insights:widgets:conditional-styling.adoc[Conditional Styling Widget Update] - Conditional styling can now be applied to edges, with the addition of an `always` xref:insights:widgets:conditional-styling.adoc#_always_option[option] in the condition dropdown.
* xref:insights:widgets:scatter-plot-widget.adoc[Added Scatter Chart Widget] - The scatter chart will provide a visual representation of the relationship between two numerical variables, allowing users to identify patterns or correlations in the data.

== Fixed issues
=== Fixed and Improved in 3.10.1

==== Functionality

* Fixed known issue where the attribute name `memberOf` was case-sensitive. It is now case-insensitive (GLE-6660).
* Fixed issue of clarity for error message/log when global `schema_change` failed for adding edge but it's relied on vertex does not exist (GLE-6751).
* Fixed issue where installation was halted due to TigerGraph disks mounted with `noexec` on AppRoot or DataRoot, preventing execution (TP-4929).
* Fixed issue where there was a delay in loading response times due to syntax detection process in GSQL (GLE-6822).
* Fixed issue were there was a GPE failure reported during query execution prompting relocation from `/tmp` to `System.TempRoot` (GLE-5536).
* Fixed issue where incorrect error response occurred when specified graph does not exist (APS-2824).
* Fixed issue where users encountered error `Vertex expansion failed: c.default.post is not a function` during Explore Neighbors operation in Insight (APS-2840).

==== Security
* Enhanced security by disabling unapproved SSL/TLS protocols of `DHE-RSA-AES128-GCM-SHA256` and `DHE-RSA-AES256-GCM-SHA384` and upgrading to TLS 1.3 in Nginx (APPS-2729).

//==== Crashes and Deadlocks
//==== Improvements
//==== Security
//==== Performance

=== Fixed and Improved in 3.10.0

==== Functionality
* Fixed issue where if the primary node is offline, access to Graph Studio was interrupted, but resumed once the primary node is back online (APPS-258).
* Fixed issue where some `GPR` and `Interpret` queries that specified the built-in `filter()` function would fail installation because of a row policy or tag filter (GLE-6448).
* Fixed issue when restarting Restpp and resulted in the task count being greater than the actual number (TP-4498).
* Fixed Issue in 3.9.3 and 3.10.0 versions could not run a GSQL query when a single node is down in a High Availability cluster. See xref:tigergraph-server:cluster-and-ha-management:ha-overview.adoc#_3_9_2_and_below[3.9.2 and below] versions workaround for more details.
* Fixed issue when changes would not save when switching to fullscreen and back in Insights (APPS-2197).
* Fixed issues where a vertex would not move after expanding in `Explore Graph` (APPS-2540).
* Fixed issue in Exception statements where if it was placed before any query-body statements, it would cause both branches of an `IF-ELSE` statement to be executed (GLE-3998).
* Fixed issue where an error in how the `ACCUM` clause is transformed, results in a transformed query with a semantic error. See xref:gsql-ref:querying:accumulators.adoc#_accumulator_types[accumulator types] for more details on valid types (GLE-5695).
* Fixed issue when parsing a negative float parameter to GSQL CLI in `{key:value}` format would create an argument error (GLE-5875).

==== Crashes and Deadlocks

* Fixed GPE crash during query execution when accumulators values are not vaild. See xref:gsql-ref:querying:accumulators.adoc#_accumulator_types[accumulator types] for more details (GLE-4411).

==== Improvements

* Improved by significantly reducing the CPU usage when a large number of loading jobs are started at the same time (TP-4159).
* Improved the write speed of loading jobs (TP-4159).
* Improved disk usage optimization by restricting a loading job in waiting status to only consumes disk resources when it actually writes data (TP-4474).
* Improved stability and cohesion of the connector and loader, which helps create better synchronization and reduces inconsistencies in the statuses (TP-4158).
* Improved significantly the pause time during backups from a few minutes to a couple of seconds, regardless of the data size. (CORE-3000).
* Improved data consistency during the backup and restore process (Core-3000).
* Improved availability when one KSL server in error state (TP-4378 & TP-4593).
* Improved the required privilege for `/rebuildnow` and `/deleted_vertex_check` making both now `Graph-level “READ DATA”` privilege and now able to run on DR cluster in CRR feature.(CORE-3291).
* Improved exception statements by adding a xref:gsql-ref:querying:exception-statements.adoc#_exception_format_not_defined_in_query[default exception format] available in cases where the exception is not defined in the query (GLE-5854)
* Improved long-running RESTPP requests and will now use less memory (CORE-3027).
* Improved log files names from `log.AUDIT` to `log.AUDIT-GSQL` (GLE-6496).
* Improved audit log `timestamp` format by extending format from `2023-12-20 14:42:50.25` to this `2023-12-20T14:42:50.243-07:00` (GLE-6395).
* Improved `userAgent` field clarity in audit logs when authenticating failed. Audit log will now record the correct user agent (GLE-6404).
* Improved audit logs by adding operating system's username to the audit log record (GLE-6394).
* Improved SearchFile experience by increasing the `GRPC_CLIENT_TIMEOUT` (APPS-2711).
* Improved functionality of the `ExprFunction` file to automatically remove the leftover “to_string” function in ExprFunction file (GLE-5834).
* Improved retention strategy for `EventQueue` that improved timely monitoring of the utilization of disk space (TP-4920).
* Improved service logs accuracy to show SSO users username in log (APPS-2496).


//(TP-4472)
//==== Security
//==== Performance

== Known Issues and Limitations

[cols="4", separator=¦ ]
|===
¦ Description ¦ Found In ¦ Workaround ¦ Fixed In

¦Running either `EXPORT GRAPH ALL` or `IMPORT GRAPH ALL` resets the TigerGraph superuser's password back to its default value.
¦3.9.1
¦ After running either command, change the superuser's password to make it secure again.
¦TBD

¦xref:backup-and-restore:database-import-export.adoc[EXPORT GRAPH ALL] does not correctly handle loading jobs containing `DELETE` statements nor graph elements with composite keys. `EXPORT GRAPH ALL` may fail if the data includes a `UDT` with a fixed string size.
¦3.2
¦
¦TBD

¦When using xref:tigergraph-server:backup-and-restore:database-import-export.adoc[IMPORT ALL] if a users schema size in the `.zip` file is exceedingly large, the import may fail with an error messages like this:

`Large catalog file key: /1/ReplicaList.json`

¦ 3.2
a¦
* 3.9 and below users need to run the import process manually by executing the GSQL scripts in the `.zip`.
* 3.10.0 and above users should xref:tigergraph-server:backup-and-restore:single-graph-import-export.adoc[import single or smaller batches of multiple graphs].
¦ TBD

a¦ If importing a role, policy, or function that has a different signature or content from the existing one, the one being imported will be skipped and not aborted.

.For example:
* If the original function is: `create function lib1.func2(int param1, float param2, string param3) returns (bool) {}`.
* And the user imports the new function: `create function lib1.func2(int param1) returns (bool) {}`. This second one will be skipped.
¦ 3.10.0
¦ Users need to re-create (delete and create) the imported role, policy, or function manually, and make sure that the importing one meets the requirements set by the existing one.
¦ TBD

a¦ xref:tigergraph-server:user-access:rbac-row-policy/row-policy-overview.adoc[Row Policy (Preview Feature)] does not yet filter or check vertex attribute data in upsert operations.

Such as,

* A query with insert statements.
* A file or Kafka loading job.
* A DDL loading request.
* Or a standard upsert request.
¦ 3.10.0
¦ Users should restrict the access of creating/running queries and loading jobs for roles related to row policy.
¦ TBD

¦ In file INPUT and OUTPUT policy, if there exists 2 path (`path1` and `path2`) in the configured policy list and `path1` is parent path of `path2`, then `path1` may not be effective.
¦ 3.2 and 3.10.0
¦ Users should avoid using paths if they are nested.

For example, avoid this scenario, path2 = `"/tmp/more"` and path1= `"/tmp"`.
¦ 3.10.1

¦ It has been observed that an issue happens when RESTPP will send a request to all gpes, and if one is down, the request sent to it will `timeout`.
Including the `consistency_check` request will also mark as `timeout`.
¦ 3.10.0
a¦
. Run `/rebuildnow` to rebuild all the segments.
+
[NOTE]
====
Running `/rebuildnow` when one gpe is down will result in the request timeout. This does not mean the request failed, instead only the currently running GPE will do the rebuild, and any rebuild requests sent to the down GPEs will result in a timeout.
====
. Run `/data_consistency_check?realtime=false` to check the consistency.
¦ TBD

¦ While running `EXPORT GRAPH` if the disk space is not enough, or the data has not been detected, the export data will get stuck loading.
¦ 3.10.0
¦ Restart all services in Admin Portal or the backend.
¦ TBD

¦ `[tg_]ExprFunction.hpp` will be automatically merged while importing single graphs. In some cases, query compilation may fail.
¦ 3.10.0
¦ See xref:tigergraph-server:backup-and-restore:single-graph-import-export.adoc#_known_issues_and_workarounds[Known Issues and Workarounds]
¦ TBD

¦ Upgrading from a previous version of TigerGraph has known issues.
¦ 3.10.0
¦ See section xref:tigergraph-server:installation:upgrade.adoc#_known_issues_and_workarounds[Known Issues and Workarounds] for more details.
¦ TBD

¦ Input Policy feature has known limitations.
¦ 3.10.0
¦ See section xref:tigergraph-server:security:gsql-file-input-policy.adoc#_limitations[Input Policy Limitations] for more details.
¦ TBD

¦ Change Data Capture (CDC) feature has known limitations.
¦ 3.10.0
¦ See section xref:tigergraph-server:system-management:change-data-capture/cdc-overview.adoc#_cdc_limitations[CDC Limitations] for more details.
¦ TBD

¦ If the `FROM` clause pattern is a multi-hop and the `ACCUM` clause reads both primitive and container type attributes or accumulators of a vertex, the internal query rewriting logic may generate an invalid rewritten output.
¦ 3.9.3
¦ This results in the error message: `It is not allowed to mix primitive types and accumulator types in GroupByAccum`.
¦ TBD

¦ Users may see a high CPU usage caused by Kafka prefetching when there is no query or posting request.
¦ 3.9.3
¦ TBD
¦ TBD

¦ GSQL query compiler may report a false error for a valid query using a vertex set variable (e.g. `Ent` in `reverse_traversal_syntax_err`) to specify the midpoint or target vertex of a path in a FROM clause pattern.
¦ TBD
¦ TBD
¦ TBD

¦ If a loading job is expected to load from a large batch of files or Kafka queues (e.g. more than 500), the job’s status may not be updated for an extended period of time.
¦ 3.9.3
¦ In this case, users should check the loader log file as an additional reference for loading status.
¦ TBD

¦ When a GPE/GSE is turned off right after initiating a loading job, the loading job is terminated internally. However, users may still observe the loading job as running on their end.
¦ 3.9.3
¦ Please see xref:gsql-ref:ddl-and-loading:running-a-loading-job.adoc[Troubleshooting Loading Job Delays] for additional details.
¦ TBD

¦ For v3.9.1 and v3.9.2 when inserting a new edge in `GPR` and `INTERPRET` mode, the GPE will print out a warning message because a discriminator string is not set for new-inserted edges. Creating an inconsistent problem in delta message for GPR and `INTERPRET` mode.
¦ 3.9.2
¦ Please see xref:gsql-ref:ddl-and-loading:running-a-loading-job.adoc[Troubleshooting Loading Job Delays] for additional details.
¦ 3.9.3

¦ GSQL `EXPORT GRAPH` may fail and cause a GPE to crash when UDT type has a fixed STRING size.
¦ TBD
¦ TBD
¦ TBD

¦ After a global loading job is running for a while a fail can be encountered when getting the loading status due to `KAFKASTRM-LL` not being online, when actually the status is online.
Then the global loading process will exit and fail the local job after timeout while waiting the global loading job to finish.
¦ TBD
¦ TBD
¦ TBD

¦ When the memory usage approaches 100%, the system may stall because the process to elect a new GSE leader did not complete correctly.
¦ TBD
¦ This lockup can be cleared by restarting the GSE.
¦ TBD

¦ If the CPU and memory utilization remain high for an extended period during a schema change on a cluster, a GSE follower could crash, if it is requested to insert data belonging to the new schema before it has finished handling the schema update.
¦ TBD
¦ TBD
¦ TBD

¦ When available memory becomes very low in a cluster and there are a large number of vertex deletions to process, some remote servers might have difficulty receiving the metadata needed to be aware of all the deletions across the full cluster. The mismatched metadata will cause the GPE to go down.
¦ TBD
¦ TBD
¦ TBD

¦ Subqueries with SET<VERTEX> parameters cannot be run in Distributed or Interpreted mode.
¦ TBD
¦ (xref:3.9@gsql-ref:querying:operators-and-expressions.adoc#_subquery_limitations[Limited Distributed model support] is added in 3.9.2.)
¦ TBD

¦ Upgrading a cluster with 10 or more nodes to v3.9.0 requires a patch.
¦ 3.9
¦ Please contact TigerGraph Support if you have a cluster this large. Clusters with nine or fewer nodes do not require the patch.
¦ 3.9.1

¦ Downsizing a cluster to have fewer nodes requires a patch.
¦ 3.9.0
¦ Please contact TigerGraph Support.
¦ TBD

¦ During peak system load, loading jobs may sometimes display an inaccurate loading status.
¦ 3.9.0
¦ This issue can be remediated by continuing to run `SHOW LOADING STATUS` periodically to display the up-to-date status.
¦ TBD

¦ When managing many loading jobs, pausing a data loading job may result in longer-than-usual response time.
¦ TBD
¦ TBD
¦ TBD

¦ Schema change jobs may fail if the server is experiencing a heavy workload.
¦ TBD
¦ To remedy this, avoid applying schema changes during peak load times.
¦ TBD

¦ User-defined Types (UDT) do not work if exceeding string size limit.
¦ TBD
¦ Avoid using UDT for variable length strings that cannot be limited by size.
¦ TBD

¦ Unable to handle the tab character `\t` properly in AVRO or Parquet file loading. It will be loaded as `\\t`.
¦ TBD
¦ TBD
¦ TBD

¦ If `System.Backup.Local.Enable` is set to `true`, this also enables a daily full backup at 12:00am UTC.
¦ 3.9.0
¦ TBD
¦ 3.9.1

¦ The data streaming connector does not handle NULL values; the connector may operate properly if a NULL value is submitted.
¦ TBD
¦ Users should replace NULL with an alternate value, such as empty string "" for STRING data, 0 for INT data, etc.  (NULL is not a valid value for the TigerGraph graph data store.)
¦ TBD

¦ Automatic message removal is an Alpha feature of the Kafka connector. It has several xref:3.9@tigergraph-server:data-loading:load-from-cloud.adoc#_known_issues_with_loading[known issues].
¦ TBD
¦ TBD
¦ TBD

¦ The `DATETIME` data type is not supported by the `PRINT … TO CSV` statement.
¦ 3.9.0
¦ TBD
¦ 3.9.1

¦ The LDAP keyword `memberOf` for declaring group hierarchy is case-sensitive.
¦ 3.9
¦ Check the case of the keywords for `memberOf`. This has been fixed in versions 3.10.1 and above.
¦ 3.10.1

|===

=== Compatibility Issues

[cols="2", separator=¦ ]
|===
¦ Description ¦ Version Introduced

¦ Users could encounter file input/output policy violations when upgrading a TigerGraph version.
See xref:tigergraph-server:security:gsql-file-input-policy.adoc#_backward_compatibility[Input policy backward compatibility.]
¦ v3.10.0

¦ When a PRINT argument is an expression, the output uses the expression as the key (label) for that output value.
To better support Antlr processing, PRINT now removes any spaces from that key. For example, `count(DISTINCT @@ids)` becomes `count(DISTINCT@@ids)`.
¦ v3.9.3+

¦ Betweenness Centrality algorithm: `reverse_edge_type (STRING)` parameter changed to `reverse_edge_type_set (SET<STRING>)`, to be consistent with `edge_type_set` and similar algorithms.
¦ v3.9.2+

¦ For vertices with string-type primary IDs, vertices whose ID is an empty string will now be rejected.
¦ v3.9.2+

¦ The default mode for the Kafka Connector changed from EOF="false" to EOF="true".
¦ v3.9.2+

¦ The default retention time for two monitoring services `Informant.RetentionPeriodDays` and `TS3.RetentionPeriodDays` has reduced from 30 to 7 days.
¦ v3.9.2+

¦ The filter for `/informant/metrics/get/cpu-memory` now accepts a list of ServiceDescriptors instead of a single ServiceDescriptor.
¦ v3.9.2+

a¦ Some user-defined functions (UDFs) may no longer be accepted due to xref:security:index.adoc#_udf_file_scanning[increased security screening].

* UDFs may no longer be called `to_string()`. This is now a built-in GSQL function.
* UDF names may no longer use the `tg_` prefix. Any user-defined function that began with `tg_` must be renamed or removed in `ExprFunctions.hpp`.
¦ v3.9+
|===

=== Deprecations

[cols="3", separator=¦ ]
|===
¦ Description ¦ Deprecated ¦ Removed

¦ The use of plaintext tokens in xref:tigergraph-server:API:authentication.adoc[authentication] is deprecated.
Use xref:tigergraph-server:user-access:jwt-token.adoc[] instead.
¦ 3.10.0
¦ 4.1


¦ The command `gbar` is removed and is no longer available.
However, if you are using a version of TigerGraph before 3.10.0 you can still use `gbar` to xref:tigergraph-server:backup-and-restore:gbar-legacy.adoc[create a backup with gbar] of the primary cluster.
See also xref:tigergraph-server:backup-and-restore:gbar-legacy.adoc[Backup and Restore with gbar] on how to create a backup.

¦ 3.7
¦ 3.10.0

¦ xref:tigergraph-server:user-access:vlac.adoc[Vertex-level Access Control (VLAC)] and xref:gsql-ref:querying:func/vertex-methods.adoc#_vlac_vertex_alias_methods_deprecated[VLAC Methods] are now deprecated and will no longer be supported.
¦ 3.10.0
¦ 4.0

¦ xref:tigergraph-server:data-loading:spark-connection-via-jdbc-driver.adoc[Spark Connection via JDBC Driver] is now deprecated and will no longer be supported.
¦ 3.10.0 
¦ TBD

¦ `Build Graph Patterns` is deprecated and will not be updated or supported and instead
we are focusing on xref:insights:widgets:index.adoc[Insights] as the tool of choice for building visual queries.
¦ v3.9.3
¦ TBD

¦ Kubernetes classic  mode (non-operator) is deprecated.
¦ v3.9
¦ TBD

¦ The `WRITE_DATA` RBAC privilege is deprecated.
¦ v3.7
¦ TBD
|===

== Release notes for previous versions
* xref:3.9@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.9]
* xref:3.8@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.8]
* xref:3.7@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.7]
* xref:3.6@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.6]
* xref:3.5@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.5]
* xref:3.4@tigergraph-server:release-notes:release-notes.adoc[Release notes - TigerGraph 3.4]
* xref:3.3@tigergraph-server:release-notes:release-notes.adoc[Release notes - TigerGraph 3.3]
* xref:3.2@tigergraph-server:release-notes:release-notes.adoc[Release notes - TigerGraph 3.2]
